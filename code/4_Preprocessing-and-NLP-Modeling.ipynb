{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocessing and NLP Modeling\n",
    "_**Author**: [Boom Devahastin Na Ayudhya](https://linkedin.com/in/boom-devahastin)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "- [Import Packages and Data](#Import-Packages-and-Data)\n",
    "- [Preprocessing](#Preprocessing)\n",
    "    - [Tweet Tokenization](#Tweet-Tokenization)\n",
    "    - [Word2Vec Vectorization](#Word2Vec-Vectorization)\n",
    "- [Modeling: NLP Classification](#Modeling:-NLP-Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adiwid\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Load data manipulation libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load text processing libraries\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim\n",
    "\n",
    "# Load visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Miscellaneous\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read clean data\n",
    "clean_tweets_df = pd.read_csv(\"./datasets/clean_tweets_20180101-20181231.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Tweet Tokenization\n",
    "\n",
    "Tokenization is the process of turning a body of text into its constituent words (i.e. tokens). To make processing easier, will create a nested list of lists which we will later iterate through.\n",
    "\n",
    "To clarify, each tweet will have a list of cleaned tokens (inner list) and will have length equal to number of tokens in that tweet. The outer list will contain these inner lists and will have length equal to the number of tweets in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\b[^\\d\\W]+\\b') # Regex here says to get full words but exclude digits\n",
    "\n",
    "# Tokenizer Tweets\n",
    "dirty_tweet_tokens = [tokenizer.tokenize(token.lower().strip()) for token in clean_tweets_df['text']]\n",
    "\n",
    "# Initialize master list to populate\n",
    "master_tweet_tokens = []\n",
    "\n",
    "# For each dirty tweet, get list of clean tokens and append to master_tweet_tokens\n",
    "for tweet in dirty_tweet_tokens:\n",
    "    # Create template for clean tokens to populate for this particular tweet\n",
    "    clean_tweet_tokens = []\n",
    "    # For each word/token in tweet, make sure it is not a stopword or URL piece or main search term \n",
    "    for word in tweet:\n",
    "        if ((word not in stopwords.words('english')) and (word not in ['http','https','www','com','@','...','â€¦','power','outage','outages','blackout'])):\n",
    "            # Now extract it into clean token list\n",
    "            clean_tweet_tokens.append(word)\n",
    "    # Append this list of tokens to the master list\n",
    "    master_tweet_tokens.append(clean_tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Vectorization\n",
    "\n",
    "Word2Vec is a model based on shallow neural networks that are trained to reconstruct the linguistic context of words. The model takes in a large corpus of text and converts it to a high-dimensional vector (typically several hundred dimensions). Each word in the corpus is assigned a corresponding vector positioned in such a way that words sharing common contexts in the corpus are close to each other in the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate Word2Vec Model from Google News\n",
    "\n",
    "The model is trained using the GoogleNews vector. Note however because the `GoogleNews-vectors-negative300.bin.gz` file is too large to be put on GitHub, you will need to save it down in your local repository one folder level up. \n",
    "\n",
    "In your terminal (iOS) or GitBash (windows), make sure you are one folder level up then run\n",
    "`curl -O https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes at least 10 minutes to load\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorization Step\n",
    "\n",
    "- We begin by defining a list of words that indicate serious/true/electrical blackout vs. list of words that refer to other non-serious blackouts (e.g. Netflix, internet, drunken night)\n",
    "- We will use these words to create vectors for word2vec.\n",
    "- Subject matter experts should refine these word lists as they see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words for SERIOUS (actual electrical) outages\n",
    "serious = ['electricity', 'electrical', 'conedison', 'con edison',\n",
    "           'generator', 'generators', 'failure', 'malfunction', 'fuse',\n",
    "           'blow', 'explode', 'power grid', 'breaker', 'loss',\n",
    "           'dark', 'darkness', 'pitch black', 'blind', 'massive',\n",
    "           'major', 'serious', 'inclement', 'surge', 'storm',\n",
    "           'solar flare', 'alert', 'light', 'lights']\n",
    "\n",
    "# List of words for NON-SERIOUS (non-electrical such as movie streaming, internet, phone, non-current \"outages\")\n",
    "non_serious = ['netflix', 'hulu', 'time warner', 'twc', 'at&t',\n",
    "               'att', 'verizon', 'tmobile', 't-mobile', 'phone',\n",
    "               'internet', 'wireless' 'conference', 'prepare', 'meeting',\n",
    "               'strength', 'training', 'discipline', 'vicpowers', 'baseball']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the corpus of words in each category, we take the word vector of each of the words and average them to gauge the overall \"sentiment\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define corpus vectorization function\n",
    "def vectorize_corpus(keyword_list):    \n",
    "    \n",
    "    # Instantiate counter for number of words in keyword_list that exists in GoogleNews word list\n",
    "    n_words = 0\n",
    "    \n",
    "    # Create template for cumulative corpus vector sum\n",
    "    corpus_vec_sum = np.zeros((1,300))                 # 300 dimensions is GoogleNews' default vector length\n",
    "    \n",
    "    # Scan through each word in corpus\n",
    "    for word in keyword_list:\n",
    "        if word in w2v_model.vocab:                    # Check if word exists in GoogleNews word list\n",
    "            word_vec = w2v_model.word_vec(word)        # If yes, vectorize it: get its 300-dimensional word vector\n",
    "            n_words +=1                                # Update counter\n",
    "            corpus_vec_sum = corpus_vec_sum + word_vec # Update cumulative vector sum for corpus\n",
    "\n",
    "    # Compute average vector by taking cumulative vector sum and dividing it by number of words traced\n",
    "    corpus_avg_vec = corpus_vec_sum/n_words\n",
    "    \n",
    "    # Squeeze this messy N-dimensional nested array object into a 1-D array to streamline future processing\n",
    "    corpus_avg_vec = np.squeeze(corpus_avg_vec)\n",
    "    \n",
    "    return(corpus_avg_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function to vectorize two corpora\n",
    "serious_vec     = vectorize_corpus(serious)\n",
    "non_serious_vec = vectorize_corpus(non_serious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling: NLP Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an ideal world, someone could manually go through each Tweet and classify whether or not it is in response to a live electrical blackout. However with thousands of tweets, this isn't a viable option. Thus, the absence of a target variable to compare means we must employ an unsupervised learning model.\n",
    "\n",
    "To tackle this classification problem, we use cosine similarity. Each tweet is vectorized in a similar manner to how we vectorized the category corpus above (i.e. vectorize each word and average them), then compare the cosine similarity scores of:\n",
    "- `tweet_avg_vec` vs. `serious_vec`\n",
    "- `tweet_avg_vec` vs. `non_serious_vec`\n",
    "\n",
    "A higher cosine similiarity score means the tweet is more likely to belong to that category. We assign a target variable value accordingly:\n",
    "- If the cosine_sim(`tweet_avg_vec`,`serious_vector`) $\\ge$  cosine_sim(`tweet_avg_vec`,`non_serious_vector`), then assign `serious_blackout = 1`.\n",
    "- If the cosine_sim(`tweet_avg_vec`,`serious_vector`) $<$  cosine_sim(`tweet_avg_vec`,`non_serious_vector`), then assign `serious_blackout = 0`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define cosine similarity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define function to compute cosine similarity score\n",
    "def cosine_sim(u, v):\n",
    "    dot_product = np.dot(u, v)\n",
    "    magnitude_product = np.sqrt(np.dot(u, u)) * np.sqrt(np.dot(v, v))\n",
    "    return (dot_product/magnitude_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify Tweets with  `serious_blackout` binary target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each tweet in our list of tweets' list of tokens\n",
    "for idx, tweet in enumerate(master_tweet_tokens):\n",
    "    \n",
    "    # Call our function to get the tweet average vector\n",
    "    tweet_avg_vec = vectorize_corpus(tweet)\n",
    "        \n",
    "    # Compare cosine similarity then assign target variable value\n",
    "    clean_tweets_df['score_serious'] = cosine_sim(tweet_avg_vec, serious_vec)\n",
    "    clean_tweets_df['score_non_serious'] = cosine_sim(tweet_avg_vec, non_serious_vec)\n",
    "    \n",
    "    if cosine_sim(tweet_avg_vec, serious_vec) >= cosine_sim(tweet_avg_vec, non_serious_vec):\n",
    "        clean_tweets_df.loc[idx,'serious_blackout'] = 1\n",
    "    else:\n",
    "        clean_tweets_df.loc[idx, 'serious_blackout'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 null or missing values in the target variable column\n",
      " \n",
      "Here are some summary statistics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serious_blackout</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>954.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.379455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.485506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       serious_blackout\n",
       "count        954.000000\n",
       "mean           0.379455\n",
       "std            0.485506\n",
       "min            0.000000\n",
       "25%            0.000000\n",
       "50%            0.000000\n",
       "75%            1.000000\n",
       "max            1.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that there are no errors in values filled\n",
    "print(\"There are\", clean_tweets_df[['serious_blackout']].isnull().sum()[0],\"null or missing values in the target variable column\")\n",
    "print(\" \")\n",
    "print(\"Here are some summary statistics:\")\n",
    "clean_tweets_df[['serious_blackout']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets_df.to_csv(\"./datasets/finalized_learned_tweets_df.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
