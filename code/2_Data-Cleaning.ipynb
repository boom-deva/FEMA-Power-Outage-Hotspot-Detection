{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning\n",
    "_**Author**: [Boom Devahastin Na Ayudhya](https://linkedin.com/in/boom-devahastin)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "1. [Import Packages and Data](#Import-Packages-and-Data)\n",
    "2. [Simulate City Location for each Tweet](#Simulate-City-Location-for-each-Tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages and Data\n",
    "Reading in the data files we generated from Section 1 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in pulled data\n",
    "city_info = pd.read_csv(\"./datasets/new_england_cities_geo-data.csv\")\n",
    "tweets_df = pd.read_csv(\"./datasets/dirty_tweets_20180101-20181231.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean these data frames by getting rid of unnamed column\n",
    "city_info.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "tweets_df.drop(columns=[\"Unnamed: 0\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate City Location for each Tweet\n",
    "- At this stage, the purpose of this project is to provide proof-of-concept.\n",
    "- Due to our numerous attempts to determine the optimal way to pull relevant Tweets with location, Twitter's API (via `twitterscrape`) has locked us out thereby preventing us from pulling any more Tweets.\n",
    "- These tweets we currently have here are relevant to power outages, but do not have informative location information.\n",
    "- Therefore, in order to proceed with modeling and geospatial visualization, we need to simulate the locations by randomly sampling from the unique list of cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique list of cities\n",
    "city_list = list(city_info['city'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly assign a city to each tweet\n",
    "np.random.seed(42)\n",
    "tweets_df[\"city\"] = np.random.choice(city_list, size=tweets_df.shape[0], replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>city</th>\n",
       "      <th>state_id</th>\n",
       "      <th>state_name</th>\n",
       "      <th>county_name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1079527340617723904</th>\n",
       "      <td>I bought a portable cell charger. It stays cha...</td>\n",
       "      <td>2018-12-30 23:59:09</td>\n",
       "      <td>Arlington</td>\n",
       "      <td>MA</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>Middlesex</td>\n",
       "      <td>42.4186</td>\n",
       "      <td>-71.1638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079527053555392513</th>\n",
       "      <td>The filthy scum media @cnn didn’t show the REA...</td>\n",
       "      <td>2018-12-30 23:58:00</td>\n",
       "      <td>Blaine</td>\n",
       "      <td>ME</td>\n",
       "      <td>Maine</td>\n",
       "      <td>Aroostook</td>\n",
       "      <td>46.4990</td>\n",
       "      <td>-67.8688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079523909312098305</th>\n",
       "      <td>Massive power outage hits southern Zim http://...</td>\n",
       "      <td>2018-12-30 23:45:31</td>\n",
       "      <td>Groton Long Point</td>\n",
       "      <td>CT</td>\n",
       "      <td>Connecticut</td>\n",
       "      <td>New London</td>\n",
       "      <td>41.3145</td>\n",
       "      <td>-72.0087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079522345134538752</th>\n",
       "      <td>Massive power outage hits southern Zim https:/...</td>\n",
       "      <td>2018-12-30 23:39:18</td>\n",
       "      <td>Shelburne</td>\n",
       "      <td>VT</td>\n",
       "      <td>Vermont</td>\n",
       "      <td>Chittenden</td>\n",
       "      <td>44.3759</td>\n",
       "      <td>-73.2265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079522317284237312</th>\n",
       "      <td>A major power outage this afternoon is impacti...</td>\n",
       "      <td>2018-12-30 23:39:11</td>\n",
       "      <td>North Woodstock</td>\n",
       "      <td>NH</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>Grafton</td>\n",
       "      <td>44.0364</td>\n",
       "      <td>-71.6895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "id                                                                       \n",
       "1079527340617723904  I bought a portable cell charger. It stays cha...   \n",
       "1079527053555392513  The filthy scum media @cnn didn’t show the REA...   \n",
       "1079523909312098305  Massive power outage hits southern Zim http://...   \n",
       "1079522345134538752  Massive power outage hits southern Zim https:/...   \n",
       "1079522317284237312  A major power outage this afternoon is impacti...   \n",
       "\n",
       "                               timestamp               city state_id  \\\n",
       "id                                                                     \n",
       "1079527340617723904  2018-12-30 23:59:09          Arlington       MA   \n",
       "1079527053555392513  2018-12-30 23:58:00             Blaine       ME   \n",
       "1079523909312098305  2018-12-30 23:45:31  Groton Long Point       CT   \n",
       "1079522345134538752  2018-12-30 23:39:18          Shelburne       VT   \n",
       "1079522317284237312  2018-12-30 23:39:11    North Woodstock       NH   \n",
       "\n",
       "                        state_name county_name      lat      lng  \n",
       "id                                                                \n",
       "1079527340617723904  Massachusetts   Middlesex  42.4186 -71.1638  \n",
       "1079527053555392513          Maine   Aroostook  46.4990 -67.8688  \n",
       "1079523909312098305    Connecticut  New London  41.3145 -72.0087  \n",
       "1079522345134538752        Vermont  Chittenden  44.3759 -73.2265  \n",
       "1079522317284237312  New Hampshire     Grafton  44.0364 -71.6895  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge rest of city_info dataframe to tweets_df\n",
    "tweets_location_df = pd.merge(tweets_df, city_info, how=\"left\", on=\"city\")\n",
    "\n",
    "# Get rid of duplicate Tweets\n",
    "tweets_location_df.drop_duplicates(subset='text', keep=\"last\", inplace=True)\n",
    "\n",
    "# Set Tweet ID and index and get rid of ID column\n",
    "tweets_location_df.set_index(tweets_location_df['id'], inplace=True)\n",
    "tweets_location_df.drop(columns=[\"id\"], inplace=True)\n",
    "\n",
    "# Peak\n",
    "tweets_location_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 954 unique Tweets remaining\n"
     ]
    }
   ],
   "source": [
    "print(\"We now have\", tweets_location_df.shape[0],\"unique Tweets remaining\")\n",
    "\n",
    "# Save as .csv\n",
    "tweets_location_df.to_csv(\"./datasets/clean_tweets_20180101-20181231.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
